{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information-Maximization Approach to Blind Separation and Blind Deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports / Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import csv\n",
    "import os\n",
    "import time as tick\n",
    "from scipy.signal import firwin, freqz, lfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializaze and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your input signals (update if needed)\n",
    "num_trials = 10\n",
    "x_chirp = np.load(\"chirp.npy\")\n",
    "x_speech, fs_speech = sf.read(\"speech.WAV\")\n",
    "x_speech = x_speech.astype(np.float64)\n",
    "\n",
    "# Example parameters\n",
    "noise_variances = [0.1, 0.3, 1.0]\n",
    "M_candidates = [64, 128, 256, 512]\n",
    "L_candidates = [4, 8, 16, 32, 64]\n",
    "eta_candidates = [3**(-i) for i in range(2,8)]\n",
    "est_cdf_candidates = ['sigmoid', 'tanh']\n",
    "\n",
    "print(fs_speech)\n",
    "plt.plot(x_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Low-Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 1000   # Cutoff frequency (Hz)\n",
    "numtaps = 501  # Filter order (should be odd)\n",
    "\n",
    "# Design FIR filter\n",
    "fir_coeff = firwin(numtaps, fc, fs=fs_speech, pass_zero=True)\n",
    "\n",
    "# Frequency response\n",
    "w, h = freqz(fir_coeff, worN=8000)\n",
    "\n",
    "# Plot Frequency Response\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(w * fs_speech / (2 * np.pi), 20 * np.log10(abs(h)), 'b')\n",
    "plt.title('FIR Lowpass Filter Frequency Response')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Gain (dB)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Memory-less Monotonic Mapping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def gaussian(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "def gaussian_derivative(x):\n",
    "    return -2 * x * np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_plant(x, fir_coeff=None):\n",
    "    \"\"\"\n",
    "    H(z) = (1 - z^-10) / (1 - z^-1)\n",
    "    y[n] = y[n-1] + x[n] - x[n-10].\n",
    "\n",
    "    else,\n",
    "    use Lowpass filters with FIR coefficients.s\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    y = np.zeros(N)\n",
    "\n",
    "    if fir_coeff is not None:\n",
    "        y = lfilter(fir_coeff, 1, x)\n",
    "    else:\n",
    "        for n in range(N):\n",
    "            if n == 0:\n",
    "                y[n] = x[n]\n",
    "            elif n < 10:\n",
    "                y[n] = y[n-1] + x[n]\n",
    "            else:\n",
    "                y[n] = y[n-1] + x[n] - x[n-10]\n",
    "    return y\n",
    "\n",
    "def add_noise(y, noise_var=0.1, seed=0):\n",
    "    \"\"\"\n",
    "    Add white Gaussian noise of variance = noise_var to y.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    noise = np.sqrt(noise_var) * np.random.randn(len(y))\n",
    "    return y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_true(M, FIR_coeffs=None):\n",
    "    if FIR_coeffs is None:\n",
    "        w_true = np.ones(10)\n",
    "    else:\n",
    "        w_true = FIR_coeffs\n",
    "    # Keeps the first 10 elements as 1 and pads the rest with zeros\n",
    "    if M <= 10:\n",
    "        return w_true\n",
    "    else:\n",
    "        w_true = np.concatenate((w_true, np.zeros(M-10)))\n",
    "        return w_true\n",
    "\n",
    "def compute_weighted_snr(w_est, FIR_coeffs=None):\n",
    "    \"\"\"\n",
    "    Weighted SNR = 10 * log10( (w_true^T w_true) / ( (w_true - w_est)^T (w_true - w_est) ) ).\n",
    "    \"\"\"\n",
    "    M = len(w_est)\n",
    "    if FIR_coeffs is not None:\n",
    "        w_true = get_w_true(M, FIR_coeffs)\n",
    "    else:\n",
    "        w_true = get_w_true(M)\n",
    "    # Pad w_est with zeros if necessary\n",
    "    if len(w_true) > len(w_est):\n",
    "        w_est = np.concatenate((w_est, np.zeros(len(w_true)-len(w_est))))\n",
    "    num = np.dot(w_true, w_true)\n",
    "    den = np.dot(w_true - w_est, w_true - w_est)\n",
    "    if den < 1e-15:\n",
    "        return 999.0\n",
    "    return 10.0 * np.log10(num / den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_DIR = \"plots\"\n",
    "\n",
    "CSV_FILENAME = \"all_results.csv\"\n",
    "\n",
    "if not os.path.exists(PLOT_DIR):\n",
    "    os.makedirs(PLOT_DIR)\n",
    "\n",
    "def plot_and_save(sig_true, sig_pred, figname, title=\"Comparison\"):\n",
    "    \"\"\"\n",
    "    Plots the true signal vs predicted, saves to figname.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(sig_true, label=\"Actual Output\", alpha=0.7)\n",
    "    plt.plot(sig_pred, label=\"Predicted Output\", alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sample index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, figname), dpi=150)\n",
    "\n",
    "def plot_comparison(sig_input, sig_noisy, sig_pred, title=\"\", max_pts=300):\n",
    "    \"\"\"\n",
    "    Plots three signals in a single figure:\n",
    "      1) Input signal\n",
    "      2) Noisy/observed output\n",
    "      3) Predicted output\n",
    "\n",
    "    max_pts: number of samples to show (for clarity).\n",
    "    \"\"\"\n",
    "    length = min(max_pts, len(sig_input), len(sig_noisy), len(sig_pred))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(sig_input[:length], label=\"Input Signal\", alpha=0.7)\n",
    "    plt.plot(sig_noisy[:length], label=\"Noisy Output\", alpha=0.7)\n",
    "    plt.plot(sig_pred[:length], label=\"Predicted Output\", alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sample index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_with_variance(time, true_signal, pred_mean, pred_std,\n",
    "                       title, xlabel=\"Sample index\", ylabel=\"Signal\",\n",
    "                       label_true=\"True Signal\", label_pred=\"Predicted Signal\", \n",
    "                       position = 'last', max_pts=500):\n",
    "    '''\n",
    "    A plotting function that plots the true signal alongside the mean\n",
    "    prediction with a shaded region indicating ± one standard deviation.\n",
    "    '''\n",
    "    if position == 'first':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        length = min(max_pts, len(time), len(true_signal), len(pred_mean), len(pred_std))\n",
    "        plt.plot(time[:length], true_signal[:length], label=label_true, color='blue', linewidth=2)\n",
    "        plt.plot(time[:length], pred_mean[:length], label=label_pred, color='red', linestyle='--', linewidth=2)\n",
    "        #plt.fill_between(time[:length], pred_mean[:length] - pred_std[:length], pred_mean[:length] + pred_std[:length],color='red', alpha=0.3, label=\"Prediction ±1 STD\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    elif position == 'last':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        length = min(max_pts, len(time), len(true_signal), len(pred_mean), len(pred_std))\n",
    "        plt.plot(time[-length:], true_signal[-length:], label=label_true, color='blue', linewidth=2)\n",
    "        plt.plot(time[-length:], pred_mean[-length:], label=label_pred, color='red', linestyle='--', linewidth=2)\n",
    "        #plt.fill_between(time[:length], pred_mean[:length] - pred_std[:length], pred_mean[:length] + pred_std[:length],color='red', alpha=0.3, label=\"Prediction ±1 STD\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"position must be either 'first' or 'last'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind Deconvolution Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blind Deconvolution Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_max_deconvolution_gaussian_pdf(x, w, filter_length=50, learning_rate=0.01, non_linearity='sigmoid'):\n",
    "    \"\"\"\n",
    "    Performs a simple InfoMax-based deconvolution on a 1D time-series signal.\n",
    "    \n",
    "    Parameters:\n",
    "        x              : 1D numpy array\n",
    "                       The observed time-series signal (assumed to be a convolution of\n",
    "                       an unknown source with a mixing kernel).\n",
    "        w              : 1D numpy array of shape (filter_length,)\n",
    "                       The initial deconvolution filter.\n",
    "        filter_length  : int, optional (default=50)\n",
    "                       The number of taps (length) of the deconvolution FIR filter.\n",
    "        num_iterations : int, optional (default=1000)\n",
    "                       Number of gradient ascent iterations.\n",
    "        learning_rate  : float, optional (default=0.01)\n",
    "                       Step size (learning rate) for gradient updates.\n",
    "                       \n",
    "    Returns:\n",
    "        w              : 1D numpy array of shape (filter_length,)\n",
    "                       The learned deconvolution filter.\n",
    "        y_final        : 1D numpy array\n",
    "                       The deconvolved output signal, computed as the sliding dot–product\n",
    "                       of the final filter with the signal.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(w) != filter_length:\n",
    "        raise ValueError(\"Filter length must match the length of the initial filter w.\")\n",
    "    \n",
    "    if len(x) != filter_length:\n",
    "        raise ValueError(\"Signal x must match the length of the initial filter w\")\n",
    "    \n",
    "    if w is None or w == np.zeros(filter_length):\n",
    "        w = np.random.randn(filter_length)\n",
    "        w = w / np.linalg.norm(w)\n",
    "\n",
    "    v = x.dot(w)\n",
    "    \n",
    "    if non_linearity == 'sigmoid':\n",
    "        activation = sigmoid\n",
    "        activation_derivative = sigmoid_derivative\n",
    "        \n",
    "        y = activation(v)\n",
    "        grad = 1/w + x * (1.0 - 2.0 * y)\n",
    "\n",
    "        w_update = w + learning_rate * grad\n",
    "\n",
    "    elif non_linearity == 'tanh':\n",
    "        activation = tanh\n",
    "        activation_derivative = tanh_derivative\n",
    "\n",
    "        y = activation(v)\n",
    "        grad = 1/w - 2 * x * y\n",
    "\n",
    "        w_update = w + learning_rate * grad\n",
    "    else:\n",
    "        raise ValueError(\"Non-linearity is either not implemented, or invalid. Try 'sigmoid' or 'tanh'.\")\n",
    "\n",
    "    return w_update, y\n",
    "\n",
    "def info_max_deconvolution_gaussian_pdf_simulation(x, filter_length=50, num_iterations=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Performs a simple InfoMax-based deconvolution on a 1D time-series signal.\n",
    "    \n",
    "    Parameters:\n",
    "      x              : 1D numpy array\n",
    "                       The observed time-series signal (assumed to be a convolution of\n",
    "                       an unknown source with a mixing kernel).\n",
    "      filter_length  : int, optional (default=50)\n",
    "                       The number of taps (length) of the deconvolution FIR filter.\n",
    "      num_iterations : int, optional (default=1000)\n",
    "                       Number of gradient ascent iterations.\n",
    "      learning_rate  : float, optional (default=0.01)\n",
    "                       Step size (learning rate) for gradient updates.\n",
    "                       \n",
    "    Returns:\n",
    "      w              : 1D numpy array of shape (filter_length,)\n",
    "                       The learned deconvolution filter.\n",
    "      y_final        : 1D numpy array\n",
    "                       The deconvolved output signal, computed as the sliding dot–product\n",
    "                       of the final filter with the signal.\n",
    "    \"\"\"\n",
    "    # Number of sliding windows (each of length filter_length)\n",
    "    T = len(x) - filter_length + 1\n",
    "    if T <= 0:\n",
    "        raise ValueError(\"Signal x must be longer than the filter length.\")\n",
    "        \n",
    "    # Create a matrix whose rows are sliding windows of x.\n",
    "    # This uses numpy's sliding_window_view (available in NumPy 1.20+).\n",
    "    try:\n",
    "        from numpy.lib.stride_tricks import sliding_window_view\n",
    "        X = sliding_window_view(x, window_shape=filter_length)\n",
    "    except ImportError:\n",
    "        # If sliding_window_view is not available, build X manually:\n",
    "        X = np.array([x[i:i+filter_length] for i in range(T)])\n",
    "    \n",
    "    # X has shape (T, filter_length)\n",
    "    \n",
    "    # Initialize filter weights (small random numbers) and normalize\n",
    "    w = np.random.randn(filter_length)\n",
    "    w = w / np.linalg.norm(w)\n",
    "    \n",
    "    # Iteratively update the filter by ascending the (approximate) output entropy\n",
    "    for it in range(num_iterations):\n",
    "        # Compute the deconvolved output using the current filter.\n",
    "        # Each element is a dot–product between w and one row (window) of X.\n",
    "        y = X.dot(w)  # y.shape is (T,)\n",
    "        \n",
    "        # Compute the sigmoid nonlinearity applied to the output\n",
    "        g_y = sigmoid(y)\n",
    "        \n",
    "        # Compute the \"score\" function, here given by (1-2*g(y))\n",
    "        error = 1.0 - 2.0 * g_y   # shape (T,)\n",
    "        \n",
    "        # Compute the gradient with respect to the filter taps.\n",
    "        # This is X^T dot error (i.e. each tap k gets gradient = sum_i error[i]*X[i,k])\n",
    "        grad = X.T.dot(error)\n",
    "        \n",
    "        # Update the filter using gradient ascent.\n",
    "        w += learning_rate * grad\n",
    "        \n",
    "        # Normalize the filter to avoid scale divergence.\n",
    "        w = w / np.linalg.norm(w)\n",
    "        \n",
    "        # (Optional) Monitor progress by computing a rough measure of output \"entropy\"\n",
    "        # Here we use -mean(log(g*(1-g))) as a proxy.\n",
    "        if (it % (num_iterations // 10) == 0) or (it == num_iterations - 1):\n",
    "            # Add a tiny constant to avoid log(0)\n",
    "            entropy_est = -np.mean(np.log(g_y * (1 - g_y) + 1e-8))\n",
    "            print(f\"Iteration {it}/{num_iterations}, entropy estimate: {entropy_est:.4f}\")\n",
    "    \n",
    "    # Compute the final deconvolved output.\n",
    "    y_final = X.dot(w)\n",
    "    return w, y_final\n",
    "\n",
    "def info_max_deconvolution_parzen_window_sampler(x, w, M=50, L=10, eta=0.01, est_cdf='sigmoid', kernel='gaussian'):\n",
    "\n",
    "    '''\n",
    "    Performs 1 iteration of the InfoMax-based deconvolution on a 1D time-series signal.\n",
    "\n",
    "    Parameters:\n",
    "        x              : 1D numpy array (filter_length+1,)\n",
    "                         The observed time-series signal (assumed to be a convolution of\n",
    "                         an unknown source with a mixing kernel).\n",
    "        w              : 1D numpy array of shape (filter_length,)\n",
    "                         The initial deconvolution filter.\n",
    "        M              : int, optional (default=50)\n",
    "                         The number of taps (length) of the deconvolution FIR filter.\n",
    "        L              : int, optional (default=10)\n",
    "                         The number of windows to use in the Parzen window entropy estimate.\n",
    "        eta            : float, optional (default=0.01)\n",
    "                         Step size (learning rate) for gradient updates.\n",
    "        non_linearity  : string, optional (default='sigmoid')\n",
    "                         The non-linearity applied to the output. Currently supports 'sigmoid' and 'tanh'.\n",
    "        kernel         : string, optional (default='gaussian')\n",
    "                         The kernel function applied to the non-linear activation. Currently supports 'gaussian'.\n",
    "\n",
    "    Returns:\n",
    "        w_new          : 1D numpy array of shape (filter_length,)\n",
    "                         The updated deconvolution filter.\n",
    "    '''\n",
    "\n",
    "    if len(w) != M:\n",
    "        raise ValueError(\"M must match the length of the initial filter w.\")\n",
    "    \n",
    "    if len(x) != M + L:\n",
    "        # There should be L+1 windows of length M in x\n",
    "        raise ValueError(\"Signal x must match the length of the initial filter M + L previous windows\")\n",
    "    \n",
    "    if w is None or np.allclose(w, 0):\n",
    "        w = np.random.randn(M)\n",
    "        w = w / np.linalg.norm(w)\n",
    "\n",
    "    # Checks for cdf mappers\n",
    "    if est_cdf == 'sigmoid':\n",
    "        activation = sigmoid\n",
    "        activation_derivative = sigmoid_derivative\n",
    "    elif est_cdf == 'tanh':\n",
    "        activation = tanh\n",
    "        activation_derivative = tanh_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Non-linearity is either not implemented, or invalid. Try 'sigmoid' or 'tanh'.\")\n",
    "    # Checks for kernel functions\n",
    "    if kernel == 'gaussian':\n",
    "        kernel = gaussian\n",
    "        kernel_derivative = gaussian_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Window is either not implemented, or invalid. Try 'gaussian'.\")\n",
    "    \n",
    "    # Get the last window\n",
    "    x_fin = x[L:L+M]\n",
    "    # Get the previous L windows (not including last, so L+1 windows total)\n",
    "    X_windows = np.lib.stride_tricks.sliding_window_view(x, M)[:L] # L x M matrix\n",
    "    # Get the prediction for the current window\n",
    "    y_fin = np.dot(x_fin,w) # 1 x 1 matrix\n",
    "    # Parallelized prediction for accelerated compute\n",
    "    Y_fin = np.full((L, 1), y_fin)  # L x 1 matrix\n",
    "    # Get the predictions for the previous L windows\n",
    "    Y_windows = np.reshape(X_windows@w, (L, 1)) # L x 1 matrix\n",
    "    # Get the cumulative density approximations for the current and previous windows\n",
    "    Z_fin = activation(Y_fin) # L x 1 matrix\n",
    "    Z_windows = activation(Y_windows) # L x 1 matrix\n",
    "    # Compute the probabilistic mass approximations for the current and previous windows\n",
    "    xt_fin = x_fin * (activation_derivative(y_fin)) # 1 x M matrix\n",
    "    Xt_fin = np.repeat(xt_fin[np.newaxis, :], L, axis=0) # L x M matrix\n",
    "    Xt_windows = X_windows * activation_derivative(Y_windows)  # L x M matrix\n",
    "    # Compute the kernel weights for the current and previous windows\n",
    "    kernel_weight = kernel_derivative(Z_fin-Z_windows) # L x 1 matrix\n",
    "    chain_rule_kernel_weight = Xt_fin - Xt_windows # L x M matrix\n",
    "    entropy_grad = np.mean(kernel_weight * chain_rule_kernel_weight, axis=0) # 1 x M matrix\n",
    "    # Update the filter weights\n",
    "    w_new = w - eta * entropy_grad\n",
    "\n",
    "    return w_new\n",
    "\n",
    "def info_max_deconvolution_parzen_window_sampler_simulation(x, M=50, L=10, eta=0.01, est_cdf='sigmoid', kernel='gaussian'):\n",
    "    \"\"\"\n",
    "    Iterates the InfoMax-based deconvolution over the entire input signal.\n",
    "    At each step a segment of length M+L is extracted and processed by\n",
    "    info_max_deconvolution_parzen_window_sampler.\n",
    "    \n",
    "    The function returns three arrays (of the same length as the input signal):\n",
    "      - weights_full: each row contains the deconvolution filter (of length M) at that iteration.\n",
    "      - predictions_full: the prediction (i.e. dot-product of the current window with the filter).\n",
    "      - errors_full: the norm difference between successive filter estimates.\n",
    "      \n",
    "    Parameters:\n",
    "        x         : 1D numpy array\n",
    "                    The observed time-series signal.\n",
    "        M         : int, optional (default=50)\n",
    "                    Number of taps (length) of the deconvolution FIR filter.\n",
    "        L         : int, optional (default=10)\n",
    "                    Number of past windows to use in the Parzen-window entropy estimate.\n",
    "        eta       : float, optional (default=0.01)\n",
    "                    Learning rate (step size).\n",
    "        est_cdf   : string, optional (default='sigmoid')\n",
    "                    Nonlinearity to use ('sigmoid' or 'tanh').\n",
    "        kernel    : string, optional (default='gaussian')\n",
    "                    Kernel function to use (currently only 'gaussian' is supported).\n",
    "                    \n",
    "    Returns:\n",
    "        weights_full    : np.ndarray of shape (N, M)\n",
    "                          Filter weight history (embedded into an array of length N).\n",
    "        predictions_full: np.ndarray of shape (N,)\n",
    "                          Predictions (the deconvolved outputs) at the embedded indices.\n",
    "        errors_full     : np.ndarray of shape (N,)\n",
    "                          Norm difference between successive filters at the embedded indices.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    N = len(x)\n",
    "    required_length = M + L\n",
    "    if N < required_length:\n",
    "        # Pad the input if it is too short for even one iteration.\n",
    "        x = np.pad(x, (0, required_length - N), mode='constant')\n",
    "        N = len(x)\n",
    "    \n",
    "    # Initialize the deconvolution filter randomly (normalized).\n",
    "    w = np.random.randn(M)\n",
    "    w = w / np.linalg.norm(w)\n",
    "    \n",
    "    # Determine the number of iterations (each iteration uses M+L samples).\n",
    "    n_iters = N - (M + L) + 1\n",
    "    \n",
    "    # Lists to store the per-iteration results.\n",
    "    weight_tracks = []\n",
    "    predictions = []\n",
    "    errors = []\n",
    "    \n",
    "    # Iterate over all valid segments of the signal.\n",
    "    for i in range(n_iters):\n",
    "        # Extract the segment of length M+L.\n",
    "        segment = x[i: i + M + L]\n",
    "        \n",
    "        # Compute the current prediction using the last M samples of the segment.\n",
    "        x_fin = segment[L:L+M]\n",
    "        current_prediction = np.dot(x_fin, w)\n",
    "        \n",
    "        # Call your per-iteration function to get the updated filter.\n",
    "        new_w = info_max_deconvolution_parzen_window_sampler(segment, w, M, L, eta, est_cdf, kernel)\n",
    "        \n",
    "        # Compute an error measure (here the change in the filter).\n",
    "        update_error = np.linalg.norm(new_w - w)\n",
    "        \n",
    "        # Save the current results.\n",
    "        predictions.append(current_prediction)\n",
    "        weight_tracks.append(new_w.copy())\n",
    "        errors.append(update_error)\n",
    "        \n",
    "        # Update the filter for the next iteration.\n",
    "        w = new_w\n",
    "    \n",
    "    # --- Embed the iteration results into full-length arrays ---\n",
    "    weights_full = np.full((N, M), np.nan)\n",
    "    predictions_full = np.full(N, np.nan)\n",
    "    errors_full = np.full(N, np.nan)\n",
    "    \n",
    "    # We assign the result from iteration i to the index corresponding to the \"center\"\n",
    "    # of the segment, i.e., i + L + M//2.\n",
    "    for i in range(n_iters):\n",
    "        center_index = i + L + M // 2\n",
    "        if center_index < N:\n",
    "            weights_full[center_index] = weight_tracks[i]\n",
    "            predictions_full[center_index] = predictions[i]\n",
    "            errors_full[center_index] = errors[i]\n",
    "    \n",
    "    return weights_full, predictions_full, errors_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Grid Search for Blind Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_deconvolution(x, d, M=50, L=10, candidate_etas=None, candidate_est_cdfs=None, num_trials=10):\n",
    "    \"\"\"\n",
    "    Grid-search for the optimal learning rate (eta) and/or non-linearity (est_cdf) for the\n",
    "    InfoMax-based deconvolution, using total normalized signal error (NMSE) as the evaluation metric.\n",
    "\n",
    "    Parameters:\n",
    "      x                 : 1D numpy array of input samples.\n",
    "      d                 : 1D numpy array of true output signals.\n",
    "      M                 : int, filter order (length of the deconvolution filter).\n",
    "      L                 : int, number of windows used in the Parzen-window entropy estimate.\n",
    "      candidate_etas    : List (or iterable) of candidate learning rates (η).  \n",
    "                          If None, a default list is used.\n",
    "      candidate_est_cdfs: List (or iterable) of candidate non-linearities (e.g., ['sigmoid', 'tanh']).\n",
    "                          If None, a default list is used.\n",
    "      num_trials        : int, number of trials per candidate configuration.\n",
    "      \n",
    "    Returns:\n",
    "      best_params       : Tuple (best_eta, best_est_cdf) that yields the lowest average NMSE.\n",
    "      best_trial_outputs: List of simulation outputs (a dict for each trial) for the best candidate.\n",
    "      performance_dict  : Dictionary mapping each candidate (eta, est_cdf) to a tuple \n",
    "                         (avg_nmse, std_nmse) computed over num_trials.\n",
    "    \"\"\"\n",
    "    # Set default candidate lists if not provided.\n",
    "    if candidate_etas is None:\n",
    "        candidate_etas = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    if candidate_est_cdfs is None:\n",
    "        candidate_est_cdfs = ['sigmoid', 'tanh']\n",
    "    \n",
    "    performance_dict = {}\n",
    "    best_avg_nmse = np.inf\n",
    "    best_params = None\n",
    "    best_trial_outputs = None\n",
    "\n",
    "    for eta_candidate in candidate_etas:\n",
    "        for est_candidate in candidate_est_cdfs:\n",
    "            trial_nmse_errors = []\n",
    "            trial_outputs = []  # To store simulation outputs for each trial.\n",
    "            \n",
    "            for trial in range(num_trials):\n",
    "                weights_full, predictions_full, errors_full = info_max_deconvolution_parzen_window_sampler_simulation(\n",
    "                    x, M=M, L=L, eta=eta_candidate, est_cdf=est_candidate, kernel='gaussian')\n",
    "                \n",
    "                # Compute the NMSE over the entire prediction sequence\n",
    "                valid_mask = ~np.isnan(predictions_full)\n",
    "                \n",
    "                x_valid = x[valid_mask]\n",
    "                y_pred = predictions_full[valid_mask]\n",
    "                d_valid = d[valid_mask]\n",
    "                #d_valid_amp = np.max(np.abs(d_valid))\n",
    "                x_valid_norm = np.linalg.norm(x_valid)\n",
    "                y_pred_norm = np.linalg.norm(y_pred)\n",
    "                d_valid_norm = np.linalg.norm(d_valid)\n",
    "\n",
    "                if len(y_pred) > 0:\n",
    "                    mse = np.mean((x_valid - y_pred*(x_valid_norm/y_pred_norm)) ** 2)\n",
    "                    norm_factor = np.mean(x_valid ** 2) + 1e-15  # Avoid division by zero\n",
    "                    nmse = mse / norm_factor\n",
    "                else:\n",
    "                    nmse = np.nan\n",
    "                \n",
    "                trial_nmse_errors.append(nmse)\n",
    "                trial_outputs.append({\n",
    "                    'weights_full': weights_full,\n",
    "                    'predictions_full': predictions_full,\n",
    "                    'errors_full': errors_full\n",
    "                })\n",
    "\n",
    "            avg_nmse = np.nanmean(trial_nmse_errors)  # Average NMSE across trials\n",
    "            std_nmse = np.nanstd(trial_nmse_errors)   # Standard deviation of NMSE\n",
    "            performance_dict[(eta_candidate, est_candidate)] = (avg_nmse, std_nmse)\n",
    "\n",
    "            print(f\"Candidate eta={eta_candidate}, est_cdf={est_candidate} --> \"\n",
    "                  f\"Avg NMSE: {avg_nmse:.6f} +/- {std_nmse:.6f}\")\n",
    "\n",
    "            if avg_nmse < best_avg_nmse:\n",
    "                best_avg_nmse = avg_nmse\n",
    "                best_params = (eta_candidate, est_candidate)\n",
    "                best_trial_outputs = trial_outputs\n",
    "\n",
    "    print(f\"\\nBest parameters: eta={best_params[0]}, est_cdf={best_params[1]} \"\n",
    "          f\"with average NMSE: {best_avg_nmse:.6f}\")\n",
    "    \n",
    "    return best_avg_nmse, best_params, best_trial_outputs, performance_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for different M and L values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was using this block to test stuff out, ignore this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid-search adapted script ---\n",
    "best_params_list = []  # will store best (eta, est_cdf) tuples for each candidate run\n",
    "\n",
    "# Prepare a DataFrame to collect performance summary.\n",
    "p1_df_deconv = pd.DataFrame(columns=['Input', 'Noise Variance', 'Model Order',\n",
    "                                       'Best Eta', 'Best est_cdf', 'W-SNR', 'NMSE'])\n",
    "\n",
    "for input_name, x_in in zip([\"speech\"], [x_speech]):\n",
    "    print(f\"\\n=== {input_name.upper()} Input ===\")\n",
    "    # Get the noise-free output from the unknown plant.\n",
    "    N = len(x_in)\n",
    "    best_y_pred = None\n",
    "    best_nmse = np.inf\n",
    "    \n",
    "    for nv in noise_variances:\n",
    "        # Add noise to the plant input.\n",
    "        x_noisy = add_noise(x_in, noise_var=nv)\n",
    "        y_noisy = unknown_plant(x_noisy, fir_coeff=fir_coeff)\n",
    "\n",
    "        y_noisy_mag = np.abs(np.max(y_noisy))\n",
    "        print(f\"  Noise variance={nv}\")\n",
    "        \n",
    "        for M in M_candidates:\n",
    "\n",
    "            for L in L_candidates:\n",
    "\n",
    "                # Use the full available data for this candidate.\n",
    "                x_w = x_in[:N]\n",
    "                y_w = y_noisy[:N]\n",
    "                \n",
    "                t_start = tick.perf_counter()\n",
    "                # Run grid search for deconvolution over the candidate learning rates and non-linearities.\n",
    "                best_avg_nmse, best_params, best_trial_outputs, performance = grid_search_deconvolution(\n",
    "                    x_w, y_w, M=M, L=L,\n",
    "                    candidate_etas=eta_candidates,\n",
    "                    candidate_est_cdfs=est_cdf_candidates,\n",
    "                    num_trials=num_trials\n",
    "                )\n",
    "                t_stop = tick.perf_counter()\n",
    "                \n",
    "                best_eta, best_est_cdf = best_params\n",
    "                best_params_list.append(best_params)\n",
    "                \n",
    "                # Each element in best_trial_outputs is a dict with keys:\n",
    "                # 'weights_full', 'predictions_full', 'errors_full'\n",
    "                trial_predictions = np.array([trial_out['predictions_full'] for trial_out in best_trial_outputs])\n",
    "                trial_weights     = np.array([trial_out['weights_full'] for trial_out in best_trial_outputs])\n",
    "                \n",
    "                # Compute the overall (across-trial) mean and std of the predictions.\n",
    "                avg_preds = np.mean(trial_predictions, axis=0)\n",
    "                std_preds = np.std(trial_predictions, axis=0)\n",
    "                \n",
    "                # Split predictions into training and validation segments (50% each).\n",
    "                train_len = int(0.5 * len(y_w))\n",
    "                y_train_true = y_w[:train_len]\n",
    "                #y_train_max = np.max(np.abs(y_train_true))\n",
    "                x_val = x_w[train_len:]\n",
    "                y_val_true   = y_w[train_len:]\n",
    "                #y_val_max = np.max(np.abs(y_val_true))\n",
    "                x_val_norm = np.linalg.norm(x_val)\n",
    "                y_val_norm = np.linalg.norm(y_val_true)\n",
    "                \n",
    "                train_preds_trials = trial_predictions[:, :train_len]\n",
    "                val_preds_trials   = trial_predictions[:, train_len:]\n",
    "                \n",
    "                avg_train_pred = np.mean(train_preds_trials, axis=0)\n",
    "                std_train_pred = np.std(train_preds_trials, axis=0)\n",
    "                avg_val_pred   = np.mean(val_preds_trials, axis=0)\n",
    "                std_val_pred   = np.std(val_preds_trials, axis=0)\n",
    "                #avg_val_pred_max = np.max(np.abs(avg_val_pred))\n",
    "                nonnan_train_pred = avg_train_pred[~np.isnan(avg_train_pred)]\n",
    "                nonnan_val_pred = avg_val_pred[~np.isnan(avg_val_pred)]\n",
    "                avg_train_pred_norm = np.linalg.norm(nonnan_train_pred)\n",
    "                avg_val_pred_norm = np.linalg.norm(nonnan_val_pred)\n",
    "                \n",
    "                print(f\"  NMSE = {best_avg_nmse:.4f}\")\n",
    "                \n",
    "                # Compute the Weighted SNR using the true impulse response and the final estimated filter.\n",
    "                final_weights = []\n",
    "                for trial_out in best_trial_outputs:\n",
    "                    weights_full = trial_out['weights_full']\n",
    "                    # Find the last (non-NaN) row of weights.\n",
    "                    valid_indices = np.where(~np.isnan(weights_full[:, 0]))[0]\n",
    "                    if len(valid_indices) > 0:\n",
    "                        final_w = weights_full[valid_indices[-1]]\n",
    "                        final_weights.append(final_w)\n",
    "                if final_weights:\n",
    "                    final_w_est = np.mean(np.array(final_weights), axis=0)\n",
    "                    w_snr = compute_weighted_snr(final_w_est, fir_coeff)\n",
    "                else:\n",
    "                    w_snr = np.nan\n",
    "                \n",
    "                print(f\"M={M}, best_eta={best_eta}, best_est_cdf={best_est_cdf}, W-SNR={w_snr:.2f} dB, \"\n",
    "                    f\"NMSE={best_avg_nmse:.6f}, Time to find best params={t_stop-t_start:.2f} s\")\n",
    "                \n",
    "                title_str_val = (f\"{input_name.upper()}, noise={nv}, M={M}, eta={best_eta}, est_cdf={best_est_cdf}\\n\"\n",
    "                                f\"W-SNR_val={w_snr:.2f} dB, NMSE={best_avg_nmse:.6f}\")\n",
    "                \n",
    "                # Plot the validation portion: true vs. predicted (with variance).\n",
    "                plot_with_variance(\n",
    "                    time=np.arange(len(y_val_true)),\n",
    "                    true_signal=x_val/ x_val_norm,\n",
    "                    pred_mean=avg_val_pred / avg_val_pred_norm,\n",
    "                    pred_std=std_val_pred / np.sqrt(avg_val_pred_norm),\n",
    "                    title=\"Validation Signal Comparison: \" + title_str_val,\n",
    "                    xlabel=\"Validation Sample Index\",\n",
    "                    ylabel=\"Signal\",\n",
    "                    position='first'\n",
    "                )\n",
    "                \n",
    "                # Plot the evolution of the weights over time.\n",
    "                trial_w_tracks = np.array([trial_out['weights_full'] for trial_out in best_trial_outputs])\n",
    "                avg_w_track = np.nanmean(trial_w_tracks, axis=0)\n",
    "                std_w_track = np.nanstd(trial_w_tracks, axis=0)\n",
    "                \n",
    "                plt.figure(figsize=(10, 5))\n",
    "                time_steps = np.arange(avg_w_track.shape[0])\n",
    "                for m in range(M):\n",
    "                    color = None\n",
    "                    plt.plot(time_steps, avg_w_track[:, m], label=f\"w[{m}]\", color=color)\n",
    "                    plt.fill_between(time_steps,\n",
    "                                    avg_w_track[:, m] - std_w_track[:, m],\n",
    "                                    avg_w_track[:, m] + std_w_track[:, m],\n",
    "                                    alpha=0.2)\n",
    "                plt.title(f\"Mean Weight Evolution Over Time (Validation) (M={M}, eta={best_eta}, est_cdf={best_est_cdf})\")\n",
    "                plt.xlabel(\"Time Steps\")\n",
    "                plt.ylabel(\"Weight Value\")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.tight_layout()\n",
    "                fig_wtrack = f\"chirp_deconv_wtrack_M_{M}_eta_{best_eta}_estcdf_{best_est_cdf}.png\"\n",
    "                plt.savefig(os.path.join(PLOT_DIR, fig_wtrack), dpi=150)\n",
    "                plt.show()\n",
    "\n",
    "                if best_avg_nmse < best_nmse:\n",
    "                    print(f\"  ** New best NMSE found: {best_avg_nmse:.6f} **\")\n",
    "                    best_nmse = best_avg_nmse\n",
    "                    best_y_pred = np.concatenate([avg_train_pred, avg_val_pred])\n",
    "                \n",
    "                # Append the results to the DataFrame.\n",
    "                new_row = pd.DataFrame([[input_name, nv, M, best_eta, best_est_cdf, w_snr, best_avg_nmse]],\n",
    "                                    columns=p1_df_deconv.columns)\n",
    "                p1_df_deconv = pd.concat([p1_df_deconv, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the best prediction for this signal.\n",
    "    out_fname = f\"speech_imax_pred.wav\"\n",
    "    nonnan_best_y_pred = best_y_pred[~np.isnan(best_y_pred)]\n",
    "    sf.write(out_fname, nonnan_best_y_pred.reshape(-1, 1), fs_speech)\n",
    "\n",
    "# Report the average best eta over all candidate runs.\n",
    "avg_best_eta = np.mean([params[0] for params in best_params_list])\n",
    "print(f\"\\nAverage best eta: {avg_best_eta:.6f}\")\n",
    "\n",
    "# Optionally, save the DataFrame of results.\n",
    "p1_df_deconv.to_csv(os.path.join(PLOT_DIR, \"deconv_grid_search_results.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the noisy signal to a file for reference.\n",
    "sf.write(\"speech_noisy.wav\", y_noisy.reshape(-1, 1), fs_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_df_deconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
